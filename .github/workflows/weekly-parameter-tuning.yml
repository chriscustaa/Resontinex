name: weekly-parameter-tuning
on:
  schedule:
    # Run every Sunday at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      force_pr:
        description: 'Force PR creation even if no improvements found'
        type: boolean
        default: false

jobs:
  parameter-optimization:
    name: Weekly Parameter Optimization
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
        
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        pip install -e .[dev]
        
    - name: Download performance baseline
      uses: actions/download-artifact@v4
      continue-on-error: true
      with:
        name: performance-baseline-main
        path: ./baseline/
        
    - name: Run parameter optimization analysis
      id: optimization
      run: |
        echo "::group::Parameter Optimization Analysis"
        
        mkdir -p optimization-results
        
        # Run parameter optimization using existing fusion_ops modules
        python -c "
        import json
        import os
        from datetime import datetime
        from fusion_ops.performance_comparison import PerformanceComparator
        from fusion_ops.budget_analysis import BudgetAnalyzer
        from fusion_ops.metrics import get_metrics_collector
        
        # Initialize analyzers
        comparator = PerformanceComparator()
        budget_analyzer = BudgetAnalyzer()
        metrics_collector = get_metrics_collector()
        
        # Load current overlay parameters
        current_overlay_path = 'configs/fusion/overlay_params.json'
        if os.path.exists(current_overlay_path):
            with open(current_overlay_path, 'r') as f:
                current_params = json.load(f)
        else:
            current_params = {
                'temperature': 0.7,
                'max_tokens': 2048,
                'top_p': 0.9,
                'frequency_penalty': 0.1,
                'presence_penalty': 0.1
            }
        
        # Load performance history if available
        baseline_path = './baseline/performance-baseline.json'
        baseline_data = None
        if os.path.exists(baseline_path):
            with open(baseline_path, 'r') as f:
                baseline_data = json.load(f)
        
        # Parameter optimization candidates
        optimization_candidates = []
        
        # Temperature optimization
        temp_variants = [0.6, 0.65, 0.7, 0.75, 0.8]
        for temp in temp_variants:
            if temp != current_params.get('temperature', 0.7):
                # Simulate performance impact (in real implementation, would run actual tests)
                latency_impact = (temp - 0.7) * 50  # Higher temp = slightly higher latency
                quality_impact = min(0.05, abs(temp - 0.7) * 0.1)  # Optimal around 0.7
                
                candidate = {
                    'parameter': 'temperature',
                    'current_value': current_params.get('temperature', 0.7),
                    'proposed_value': temp,
                    'estimated_latency_change_ms': latency_impact,
                    'estimated_quality_change': quality_impact,
                    'confidence': 0.8
                }
                optimization_candidates.append(candidate)
        
        # Max tokens optimization
        token_variants = [1024, 1536, 2048, 2560, 3072]
        for tokens in token_variants:
            if tokens != current_params.get('max_tokens', 2048):
                # Simulate performance impact
                token_ratio = tokens / 2048
                latency_impact = (token_ratio - 1) * 200  # More tokens = higher latency
                cost_impact = (token_ratio - 1) * 15  # More tokens = higher cost
                
                candidate = {
                    'parameter': 'max_tokens',
                    'current_value': current_params.get('max_tokens', 2048),
                    'proposed_value': tokens,
                    'estimated_latency_change_ms': latency_impact,
                    'estimated_cost_change_pct': cost_impact,
                    'confidence': 0.9
                }
                optimization_candidates.append(candidate)
        
        # Top-p optimization
        top_p_variants = [0.85, 0.9, 0.95]
        for top_p in top_p_variants:
            if top_p != current_params.get('top_p', 0.9):
                # Simulate performance impact
                diversity_impact = (top_p - 0.9) * 0.02  # Higher top_p = more diversity
                
                candidate = {
                    'parameter': 'top_p',
                    'current_value': current_params.get('top_p', 0.9),
                    'proposed_value': top_p,
                    'estimated_diversity_change': diversity_impact,
                    'confidence': 0.7
                }
                optimization_candidates.append(candidate)
        
        # Filter to top recommendations
        best_candidates = []
        for candidate in optimization_candidates:
            # Score based on estimated improvements and confidence
            score = 0
            
            if 'estimated_latency_change_ms' in candidate:
                if candidate['estimated_latency_change_ms'] < 0:  # Improvement
                    score += abs(candidate['estimated_latency_change_ms']) * 0.1
                else:  # Regression
                    score -= candidate['estimated_latency_change_ms'] * 0.2
            
            if 'estimated_quality_change' in candidate:
                score += candidate['estimated_quality_change'] * 100
                
            if 'estimated_cost_change_pct' in candidate:
                if candidate['estimated_cost_change_pct'] < 0:  # Cost reduction
                    score += abs(candidate['estimated_cost_change_pct']) * 2
                else:  # Cost increase
                    score -= candidate['estimated_cost_change_pct'] * 1.5
            
            score *= candidate.get('confidence', 1.0)
            
            if score > 5:  # Threshold for recommendation
                candidate['optimization_score'] = score
                best_candidates.append(candidate)
        
        # Sort by optimization score
        best_candidates.sort(key=lambda x: x['optimization_score'], reverse=True)
        
        # Take top 3 recommendations
        recommendations = best_candidates[:3]
        
        optimization_result = {
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'current_parameters': current_params,
            'recommendations': recommendations,
            'baseline_performance': baseline_data.get('metrics', {}) if baseline_data else {},
            'analysis_metadata': {
                'candidates_evaluated': len(optimization_candidates),
                'recommendations_count': len(recommendations),
                'has_baseline': baseline_data is not None
            }
        }
        
        # Save optimization results
        with open('optimization-results/parameter-optimization.json', 'w') as f:
            json.dump(optimization_result, f, indent=2)
        
        print(f'Parameter optimization complete: {len(recommendations)} recommendations')
        
        if recommendations:
            print('TOP_RECOMMENDATION=True')
            best_rec = recommendations[0]
            print(f'BEST_PARAM={best_rec[\"parameter\"]}')
            print(f'BEST_VALUE={best_rec[\"proposed_value\"]}')
            print(f'OPTIMIZATION_SCORE={best_rec[\"optimization_score\"]:.2f}')
        else:
            print('TOP_RECOMMENDATION=False')
            print('No significant optimization opportunities found')
        " 2>&1 | tee optimization_analysis_output.txt
        
        # Extract results for next steps
        if grep -q "TOP_RECOMMENDATION=True" optimization_analysis_output.txt; then
          echo "has_recommendations=true" >> $GITHUB_OUTPUT
          best_param=$(grep "BEST_PARAM=" optimization_analysis_output.txt | cut -d'=' -f2)
          best_value=$(grep "BEST_VALUE=" optimization_analysis_output.txt | cut -d'=' -f2)
          opt_score=$(grep "OPTIMIZATION_SCORE=" optimization_analysis_output.txt | cut -d'=' -f2)
          
          echo "best_parameter=$best_param" >> $GITHUB_OUTPUT
          echo "best_value=$best_value" >> $GITHUB_OUTPUT
          echo "optimization_score=$opt_score" >> $GITHUB_OUTPUT
          echo "✅ Optimization recommendations generated"
        else
          echo "has_recommendations=false" >> $GITHUB_OUTPUT
          echo "ℹ️ No significant optimizations found"
        fi
        
        echo "::endgroup::"
        
    - name: Create optimization branch
      if: steps.optimization.outputs.has_recommendations == 'true' || github.event.inputs.force_pr == 'true'
      run: |
        echo "::group::Creating Optimization Branch"
        
        optimization_date=$(date +%Y-%m-%d)
        branch_name="auto/parameter-optimization-$optimization_date"
        
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        git checkout -b "$branch_name"
        echo "branch_name=$branch_name" >> $GITHUB_ENV
        
        echo "✅ Created branch: $branch_name"
        echo "::endgroup::"
        
    - name: Apply parameter optimizations
      if: steps.optimization.outputs.has_recommendations == 'true'
      run: |
        echo "::group::Applying Parameter Optimizations"
        
        # Apply the top recommendation
        python -c "
        import json
        import os
        
        # Load optimization results
        with open('optimization-results/parameter-optimization.json', 'r') as f:
            results = json.load(f)
        
        if not results['recommendations']:
            print('No recommendations to apply')
            exit(0)
        
        # Load current parameters
        overlay_params_path = 'configs/fusion/overlay_params.json'
        if os.path.exists(overlay_params_path):
            with open(overlay_params_path, 'r') as f:
                current_params = json.load(f)
        else:
            current_params = {}
        
        # Apply top recommendation
        top_rec = results['recommendations'][0]
        param_name = top_rec['parameter']
        new_value = top_rec['proposed_value']
        
        print(f'Applying optimization: {param_name} = {new_value}')
        
        # Update parameters
        current_params[param_name] = new_value
        
        # Add optimization metadata
        if 'optimization_history' not in current_params:
            current_params['optimization_history'] = []
        
        current_params['optimization_history'].append({
            'timestamp': results['timestamp'],
            'parameter': param_name,
            'previous_value': top_rec['current_value'],
            'new_value': new_value,
            'optimization_score': top_rec['optimization_score'],
            'confidence': top_rec.get('confidence', 1.0)
        })
        
        # Keep only last 10 optimization history entries
        current_params['optimization_history'] = current_params['optimization_history'][-10:]
        
        # Ensure directory exists
        os.makedirs(os.path.dirname(overlay_params_path), exist_ok=True)
        
        # Save updated parameters
        with open(overlay_params_path, 'w') as f:
            json.dump(current_params, f, indent=2)
        
        print(f'✅ Parameters updated in {overlay_params_path}')
        "
        
        echo "::endgroup::"
        
    - name: Commit changes
      if: steps.optimization.outputs.has_recommendations == 'true' || github.event.inputs.force_pr == 'true'
      run: |
        git add optimization-results/
        git add configs/fusion/overlay_params.json
        
        optimization_date=$(date +%Y-%m-%d)
        
        if [ "${{ steps.optimization.outputs.has_recommendations }}" == "true" ]; then
          git commit -m "chore: weekly parameter optimization $optimization_date" \
            -m "Optimized ${{ steps.optimization.outputs.best_parameter }} to ${{ steps.optimization.outputs.best_value }}" \
            -m "Optimization score: ${{ steps.optimization.outputs.optimization_score }}" \
            -m "Auto-generated by weekly parameter tuning workflow"
        else
          git commit -m "chore: weekly parameter optimization $optimization_date" \
            -m "No significant optimizations found - updating analysis results only" \
            -m "Auto-generated by weekly parameter tuning workflow"
        fi
        
        git push origin "${{ env.branch_name }}"
        
    - name: Create pull request
      if: steps.optimization.outputs.has_recommendations == 'true' || github.event.inputs.force_pr == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const hasRecommendations = '${{ steps.optimization.outputs.has_recommendations }}' === 'true';
          const bestParam = '${{ steps.optimization.outputs.best_parameter }}';
          const bestValue = '${{ steps.optimization.outputs.best_value }}';
          const optScore = '${{ steps.optimization.outputs.optimization_score }}';
          
          let prTitle, prBody;
          
          if (hasRecommendations) {
            prTitle = `🎯 Weekly Parameter Optimization: ${bestParam} → ${bestValue}`;
            prBody = `## 🎯 Weekly Parameter Optimization

**Optimization Summary:**
- **Parameter:** \`${bestParam}\`
- **Proposed Value:** \`${bestValue}\`
- **Optimization Score:** ${optScore}

### 📊 Analysis Results
Parameter optimization analysis identified improvement opportunities.

### ✅ Next Steps
1. Review the proposed parameter changes
2. Run integration tests to validate performance impact
3. Monitor fusion effectiveness after merge

*Auto-generated by RESONTINEX Weekly Parameter Tuning*`;
          } else {
            prTitle = '📊 Weekly Parameter Analysis - No Optimizations';
            prBody = `## 📊 Weekly Parameter Analysis

**Status:** No significant optimization opportunities found

### 🔍 Analysis Summary
The weekly parameter optimization analysis completed successfully but found no parameter changes that would provide meaningful performance improvements.

### 📈 Current Performance
Current overlay parameters appear to be performing optimally based on recent metrics.

*Auto-generated by RESONTINEX Weekly Parameter Tuning*`;
          }
          
          const { data: pullRequest } = await github.rest.pulls.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: prTitle,
            head: '${{ env.branch_name }}',
            base: 'main',
            body: prBody,
            draft: false
          });
          
          console.log(`Created PR: ${pullRequest.html_url}`);
          
          await github.rest.issues.addLabels({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: pullRequest.number,
            labels: hasRecommendations ? ['optimization', 'automated', 'parameter-tuning'] : ['analysis', 'automated', 'no-change']
          });
          
    - name: Upload optimization artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: parameter-optimization-${{ github.run_number }}
        path: |
          optimization-results/
          optimization_analysis_output.txt
        retention-days: 30